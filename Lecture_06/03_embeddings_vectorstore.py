"""
RAG Ğ¨Ğ°Ğ³ 3: Embeddings Ğ¸ Vector Store
====================================
Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.
"""
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS, Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

load_dotenv()

# ============================================================
# 1. EMBEDDING ĞœĞĞ”Ğ•Ğ›Ğ˜
# ============================================================
print("="*60)
print("1ï¸âƒ£ EMBEDDING ĞœĞĞ”Ğ•Ğ›Ğ˜ OpenAI")
print("="*60)
print("""
   text-embedding-3-small  - Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ ($0.02/1M Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
   text-embedding-3-large  - Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğµ ($0.13/1M Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)
   text-embedding-ada-002  - legacy Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
""")

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ small - Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
print("âœ… Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼: text-embedding-3-small")

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ embedding
test_text = "Ğ“Ğ°Ñ€Ñ€Ğ¸ ĞŸĞ¾Ñ‚Ñ‚ĞµÑ€ - Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¶Ğ¸Ğ»"
vector = embeddings.embed_query(test_text)
print(f"\nğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°: {len(vector)}")
print(f"   ĞŸĞµÑ€Ğ²Ñ‹Ğµ 5 Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹: {vector[:5]}")


# ============================================================
# 2. ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢ĞĞ’
# ============================================================
print("\n" + "="*60)
print("2ï¸âƒ£ ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢ĞĞ’")
print("="*60)

documents = []
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

for file in sorted(Path("data").glob("*.txt"))[:2]:  # Ğ‘ĞµÑ€ĞµĞ¼ 2 ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾
    name = file.stem.split(']_')[-1].replace('_', ' ')
    text = file.read_text(encoding='utf-8')
    chunks = splitter.split_text(text)
    
    for i, chunk in enumerate(chunks):
        documents.append(Document(
            page_content=chunk,
            metadata={"title": name, "chunk_id": i}
        ))
    print(f"ğŸ“– {name}: {len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²")

print(f"\nâœ… Ğ’ÑĞµĞ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: {len(documents)}")


# ============================================================
# 3. VECTOR STORE: FAISS
# ============================================================
print("\n" + "="*60)
print("3ï¸âƒ£ FAISS - Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Vector Store")
print("="*60)
print("""
   âœ… ĞÑ‡ĞµĞ½ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
   âœ… Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾
   âœ… ĞĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
   âŒ ĞÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ/Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒ
""")

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°
print("\nğŸ”„ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ FAISS Ğ¸Ğ½Ğ´ĞµĞºÑĞ°...")
faiss_store = FAISS.from_documents(documents, embeddings)
print("âœ… FAISS Ğ¸Ğ½Ğ´ĞµĞºÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½!")

# Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ¸ÑĞº
faiss_store.save_local("./faiss_demo")
print("ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: ./faiss_demo")

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ Ğ´Ğ¸ÑĞºĞ°
# faiss_loaded = FAISS.load_local("./faiss_demo", embeddings, allow_dangerous_deserialization=True)


# ============================================================
# 4. VECTOR STORE: CHROMA
# ============================================================
print("\n" + "="*60)
print("4ï¸âƒ£ CHROMA - ĞŸĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Vector Store")
print("="*60)
print("""
   âœ… ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
   âœ… Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼
   âœ… Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ´Ğ»Ñ production
   âŒ ĞœĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ FAISS
""")

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ (Ğ·Ğ°ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ)
# chroma_store = Chroma.from_documents(
#     documents, 
#     embeddings,
#     persist_directory="./chroma_demo"
# )


# ============================================================
# 5. ĞŸĞĞ˜Ğ¡Ğš
# ============================================================
print("\n" + "="*60)
print("5ï¸âƒ£ Ğ¡Ğ•ĞœĞĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞŸĞĞ˜Ğ¡Ğš")
print("="*60)

queries = [
    "ĞšÑ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ’Ğ¾Ğ»Ğ´ĞµĞ¼Ğ¾Ñ€Ñ‚?",
    "ĞšĞ°Ğº Ğ“Ğ°Ñ€Ñ€Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ğ» Ğ² Ğ¥Ğ¾Ğ³Ğ²Ğ°Ñ€Ñ‚Ñ?",
    "Ğ”Ñ€ÑƒĞ·ÑŒÑ Ğ“Ğ°Ñ€Ñ€Ğ¸ ĞŸĞ¾Ñ‚Ñ‚ĞµÑ€Ğ°",
]

for query in queries:
    print(f"\nâ“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: {query}")
    print("-"*40)
    
    # ĞŸĞ¾Ğ¸ÑĞº Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹
    results = faiss_store.similarity_search_with_score(query, k=2)
    
    for doc, score in results:
        print(f"ğŸ“„ [{doc.metadata['title']}] score={score:.3f}")
        print(f"   {doc.page_content[:150]}...")


# ============================================================
# Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• VECTOR STORES
# ============================================================
print("\n" + "="*60)
print("ğŸ“Š Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• VECTOR STORES")
print("="*60)
print("""
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Store      â”‚  Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒâ”‚  Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ â”‚  Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµâ”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  FAISS      â”‚  â­â­â­   â”‚  Ğ ÑƒÑ‡Ğ½Ğ¾Ğµ     â”‚  Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ    â”‚
   â”‚  Chroma     â”‚  â­â­     â”‚  ĞĞ²Ñ‚Ğ¾       â”‚  Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ    â”‚
   â”‚  Pinecone   â”‚  â­â­     â”‚  ĞĞ±Ğ»Ğ°ĞºĞ¾     â”‚  Production   â”‚
   â”‚  Qdrant     â”‚  â­â­â­   â”‚  ĞĞ²Ñ‚Ğ¾       â”‚  Production   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
