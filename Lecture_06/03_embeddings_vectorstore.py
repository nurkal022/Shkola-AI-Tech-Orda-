"""
RAG –®–∞–≥ 3: Embeddings –∏ Vector Store
====================================
–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.
"""
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

load_dotenv()

# ============================================================
# 1. EMBEDDING –ú–û–î–ï–õ–ò
# ============================================================
print("="*60)
print("1Ô∏è‚É£ EMBEDDING –ú–û–î–ï–õ–ò OpenAI")
print("="*60)
print("""
   text-embedding-3-small  - –±—ã—Å—Ç—Ä–µ–µ, –¥–µ—à–µ–≤–ª–µ ($0.02/1M —Ç–æ–∫–µ–Ω–æ–≤)
   text-embedding-3-large  - —Ç–æ—á–Ω–µ–µ, –¥–æ—Ä–æ–∂–µ ($0.13/1M —Ç–æ–∫–µ–Ω–æ–≤)
   text-embedding-ada-002  - legacy –º–æ–¥–µ–ª—å
""")

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º: text-embedding-3-small")

# –ü—Ä–∏–º–µ—Ä embedding
test_text = "–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä - –º–∞–ª—å—á–∏–∫ –∫–æ—Ç–æ—Ä—ã–π –≤—ã–∂–∏–ª"
vector = embeddings.embed_query(test_text)
print(f"\nüìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞: {len(vector)}")
print(f"   –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {vector[:5]}")


# ============================================================
# 2. –ü–û–î–ì–û–¢–û–í–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í (—Ç–æ–ª—å–∫–æ 1 –∫–Ω–∏–≥–∞ –¥–ª—è –¥–µ–º–æ)
# ============================================================
print("\n" + "="*60)
print("2Ô∏è‚É£ –ü–û–î–ì–û–¢–û–í–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í")
print("="*60)

documents = []
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –∫–Ω–∏–≥—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–µ–º–æ
file = list(Path("data").glob("*.txt"))[0]
name = file.stem.split(']_')[-1].replace('_', ' ')
text = file.read_text(encoding='utf-8')
chunks = splitter.split_text(text)

# –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100 —á–∞–Ω–∫–æ–≤ –¥–ª—è –¥–µ–º–æ (–∏–∑–±–µ–≥–∞–µ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤)
for i, chunk in enumerate(chunks[:100]):
    documents.append(Document(
        page_content=chunk,
        metadata={"title": name, "chunk_id": i}
    ))

print(f"üìñ {name}: {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤—Å–µ–≥–æ")
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –¥–µ–º–æ: {len(documents)} —á–∞–Ω–∫–æ–≤")


# ============================================================
# 3. –°–û–ó–î–ê–ù–ò–ï FAISS –ò–ù–î–ï–ö–°–ê
# ============================================================
print("\n" + "="*60)
print("3Ô∏è‚É£ FAISS - –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞")
print("="*60)

print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
faiss_store = FAISS.from_documents(documents, embeddings)
print("‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω!")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
faiss_store.save_local("./faiss_demo")
print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: ./faiss_demo")


# ============================================================
# 4. –ü–û–ò–°–ö –ë–ï–ó –ì–ï–ù–ï–†–ê–¶–ò–ò - –ø—Ä–æ—Å—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
# ============================================================
print("\n" + "="*60)
print("4Ô∏è‚É£ –ü–û–ò–°–ö –î–û–ö–£–ú–ï–ù–¢–û–í (–±–µ–∑ LLM)")
print("="*60)

queries = [
    "–ö—Ç–æ —Ç–∞–∫–æ–π –í–æ–ª–¥–µ–º–æ—Ä—Ç?",
    "–•–æ–≥–≤–∞—Ä—Ç—Å",
    "–î–∞–º–±–ª–¥–æ—Ä",
]

for query in queries:
    print(f"\n‚ùì –ó–∞–ø—Ä–æ—Å: {query}")
    print("-"*50)
    
    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã
    docs = faiss_store.similarity_search(query, k=3)
    
    for i, doc in enumerate(docs, 1):
        print(f"\nüìÑ –î–æ–∫—É–º–µ–Ω—Ç {i}:")
        print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata['title']}, chunk #{doc.metadata['chunk_id']}")
        print(f"   –¢–µ–∫—Å—Ç: {doc.page_content[:200]}...")


# ============================================================
# 5. –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –ü–û–ò–°–ö–ê
# ============================================================
print("\n" + "="*60)
print("5Ô∏è‚É£ –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –ü–û–ò–°–ö–ê")
print("="*60)

query = "–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä –≤–æ–ª—à–µ–±–Ω–∏–∫"

# –ú–µ—Ç–æ–¥ 1: similarity_search - –±–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
print("\nüîç 1. similarity_search (–±–∞–∑–æ–≤—ã–π)")
docs1 = faiss_store.similarity_search(query, k=3)
for doc in docs1:
    print(f"   ‚Ä¢ chunk #{doc.metadata['chunk_id']}: {doc.page_content[:80]}...")

# –ú–µ—Ç–æ–¥ 2: similarity_search_with_score - —Å –æ—Ü–µ–Ω–∫–æ–π —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
print("\nüîç 2. similarity_search_with_score (—Å –æ—Ü–µ–Ω–∫–æ–π)")
docs2 = faiss_store.similarity_search_with_score(query, k=3)
for doc, score in docs2:
    print(f"   ‚Ä¢ score={score:.3f}, chunk #{doc.metadata['chunk_id']}: {doc.page_content[:60]}...")

# –ú–µ—Ç–æ–¥ 3: max_marginal_relevance_search (MMR) - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
print("\nüîç 3. max_marginal_relevance_search (MMR - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)")
docs3 = faiss_store.max_marginal_relevance_search(query, k=3, fetch_k=10)
for doc in docs3:
    print(f"   ‚Ä¢ chunk #{doc.metadata['chunk_id']}: {doc.page_content[:80]}...")

# –ú–µ—Ç–æ–¥ 4: similarity_search_with_relevance_scores - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
print("\nüîç 4. similarity_search_with_relevance_scores (0-1 –æ—Ü–µ–Ω–∫–∞)")
docs4 = faiss_store.similarity_search_with_relevance_scores(query, k=3)
for doc, score in docs4:
    print(f"   ‚Ä¢ relevance={score:.3f}, chunk #{doc.metadata['chunk_id']}: {doc.page_content[:60]}...")


# ============================================================
# 6. –ü–û–ò–°–ö –ü–û –í–ï–ö–¢–û–†–£ –ù–ê–ü–†–Ø–ú–£–Æ
# ============================================================
print("\n" + "="*60)
print("6Ô∏è‚É£ –ü–û–ò–°–ö –ü–û –í–ï–ö–¢–û–†–£ –ù–ê–ü–†–Ø–ú–£–Æ")
print("="*60)

# –°–æ–∑–¥–∞–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞ –≤—Ä—É—á–Ω—É—é
query_vector = embeddings.embed_query("–º–∞–≥–∏—è –∏ –≤–æ–ª—à–µ–±—Å—Ç–≤–æ")
print(f"üìä –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞: {len(query_vector)} –∏–∑–º–µ—Ä–µ–Ω–∏–π")

# –ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä—É
docs5 = faiss_store.similarity_search_by_vector(query_vector, k=2)
for doc in docs5:
    print(f"   ‚Ä¢ {doc.page_content[:100]}...")


# ============================================================
# –ò–¢–û–ì
# ============================================================
print("\n" + "="*60)
print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í")
print("="*60)
print("""
   similarity_search              ‚Üí –ë–∞–∑–æ–≤—ã–π, –±—ã—Å—Ç—Ä—ã–π
   similarity_search_with_score   ‚Üí + —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
   similarity_search_with_relevance_scores ‚Üí + —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å 0-1
   max_marginal_relevance_search  ‚Üí + —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
   similarity_search_by_vector    ‚Üí –ü–æ–∏—Å–∫ –ø–æ –≥–æ—Ç–æ–≤–æ–º—É –≤–µ–∫—Ç–æ—Ä—É
""")
