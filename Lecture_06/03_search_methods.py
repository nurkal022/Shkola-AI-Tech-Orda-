"""
RAG: Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°
=============================
Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ‘Ğ•Ğ— Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.
ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹.
"""
from pathlib import Path
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

# ============================================================
# ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ Ğ”ĞĞĞĞ«Ğ¥
# ============================================================
print("="*60)
print("ğŸ“š ĞŸĞĞ”Ğ“ĞĞ¢ĞĞ’ĞšĞ Ğ”ĞĞĞĞ«Ğ¥")
print("="*60)

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºĞ½Ğ¸Ğ³Ñƒ
file = list(Path("data").glob("*.txt"))[0]
text = file.read_text(encoding='utf-8')

# Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_text(text)[:200]  # 200 Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾

print(f"ğŸ“– Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾: {len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²")

# Embeddings Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ FAISS Ğ¸Ğ½Ğ´ĞµĞºÑ
print("ğŸ”„ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°...")
vectorstore = FAISS.from_texts(chunks, embeddings)
print("âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!\n")


# ============================================================
# 1. KEYWORD SEARCH (BM25)
# ============================================================
def keyword_search(query: str, k: int = 5):
    """
    BM25 - ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼.
    Ğ˜Ñ‰ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ.
    """
    # Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    tokenized_chunks = [chunk.lower().split() for chunk in chunks]
    bm25 = BM25Okapi(tokenized_chunks)
    
    # ĞŸĞ¾Ğ¸ÑĞº
    tokenized_query = query.lower().split()
    scores = bm25.get_scores(tokenized_query)
    
    # Ğ¢Ğ¾Ğ¿ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
    
    results = []
    for idx in top_indices:
        results.append({
            "chunk_id": idx,
            "score": scores[idx],
            "text": chunks[idx]
        })
    return results


# ============================================================
# 2. VECTOR SEARCH (Semantic)
# ============================================================
def vector_search(query: str, k: int = 5):
    """
    Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº - ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾.
    ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ, Ğ´Ğ°Ğ¶Ğµ Ğ±ĞµĞ· Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ².
    """
    results = vectorstore.similarity_search_with_score(query, k=k)
    
    return [{
        "chunk_id": chunks.index(doc.page_content) if doc.page_content in chunks else -1,
        "score": float(score),
        "text": doc.page_content
    } for doc, score in results]


# ============================================================
# 3. MMR SEARCH (Diversity)
# ============================================================
def mmr_search(query: str, k: int = 5):
    """
    MMR - Maximum Marginal Relevance.
    Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼.
    """
    results = vectorstore.max_marginal_relevance_search(
        query, k=k, fetch_k=20, lambda_mult=0.5
    )
    
    return [{
        "chunk_id": chunks.index(doc.page_content) if doc.page_content in chunks else -1,
        "text": doc.page_content
    } for doc in results]


# ============================================================
# 4. HYBRID SEARCH (Keyword + Vector)
# ============================================================
def hybrid_search(query: str, k: int = 5, alpha: float = 0.5):
    """
    Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº - ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ BM25 Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾.
    alpha: 0 = Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ BM25, 1 = Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹
    """
    # BM25 scores
    tokenized_chunks = [chunk.lower().split() for chunk in chunks]
    bm25 = BM25Okapi(tokenized_chunks)
    bm25_scores = bm25.get_scores(query.lower().split())
    
    # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ BM25
    max_bm25 = max(bm25_scores) if max(bm25_scores) > 0 else 1
    bm25_norm = [s / max_bm25 for s in bm25_scores]
    
    # Vector scores
    vector_results = vectorstore.similarity_search_with_score(query, k=len(chunks))
    vector_scores = {doc.page_content: 1 - (score / 2) for doc, score in vector_results}  # Ğ˜Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼
    
    # ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼
    combined = []
    for i, chunk in enumerate(chunks):
        vec_score = vector_scores.get(chunk, 0)
        hybrid_score = (1 - alpha) * bm25_norm[i] + alpha * vec_score
        combined.append((i, hybrid_score, chunk))
    
    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼
    combined.sort(key=lambda x: x[1], reverse=True)
    
    return [{
        "chunk_id": idx,
        "score": score,
        "text": text
    } for idx, score, text in combined[:k]]


# ============================================================
# Ğ”Ğ•ĞœĞĞĞ¡Ğ¢Ğ ĞĞ¦Ğ˜Ğ¯
# ============================================================
def show_results(results, method_name):
    """ĞšÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"""
    print(f"\n{'='*60}")
    print(f"ğŸ” {method_name}")
    print(f"{'='*60}")
    
    for i, r in enumerate(results, 1):
        score_str = f" (score: {r['score']:.3f})" if 'score' in r else ""
        print(f"\nğŸ“„ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ {i}{score_str}")
        print(f"   Chunk ID: {r['chunk_id']}")
        print(f"   Ğ¢ĞµĞºÑÑ‚: {r['text'][:300]}...")


# ============================================================
# Ğ¢Ğ•Ğ¡Ğ¢ 1: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ (ĞµÑÑ‚ÑŒ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ)
# ============================================================
print("\n" + "="*60)
print("ğŸ§ª Ğ¢Ğ•Ğ¡Ğ¢ 1: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ")
print("="*60)

query1 = "Ğ“Ğ°Ñ€Ñ€Ğ¸ ĞŸĞ¾Ñ‚Ñ‚ĞµÑ€"
print(f"\nâ“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: '{query1}'")

show_results(keyword_search(query1, k=3), "1ï¸âƒ£ KEYWORD (BM25)")
show_results(vector_search(query1, k=3), "2ï¸âƒ£ VECTOR (Semantic)")
show_results(mmr_search(query1, k=3), "3ï¸âƒ£ MMR (Diversity)")
show_results(hybrid_search(query1, k=3), "4ï¸âƒ£ HYBRID (BM25 + Vector)")


# ============================================================
# Ğ¢Ğ•Ğ¡Ğ¢ 2: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ (ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹)
# ============================================================
print("\n\n" + "="*60)
print("ğŸ§ª Ğ¢Ğ•Ğ¡Ğ¢ 2: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ (ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹)")
print("="*60)

query2 = "Ğ¼Ğ°Ğ»ÑŒÑ‡Ğ¸Ğº Ğ²Ğ¾Ğ»ÑˆĞµĞ±Ğ½Ğ¸Ğº"  # ĞĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ², Ğ½Ğ¾ ĞµÑÑ‚ÑŒ ÑĞ¼Ñ‹ÑĞ»
print(f"\nâ“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: '{query2}'")

show_results(keyword_search(query2, k=3), "1ï¸âƒ£ KEYWORD (BM25)")
show_results(vector_search(query2, k=3), "2ï¸âƒ£ VECTOR (Semantic)")


# ============================================================
# Ğ¢Ğ•Ğ¡Ğ¢ 3: Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ
# ============================================================
print("\n\n" + "="*60)
print("ğŸ§ª Ğ¢Ğ•Ğ¡Ğ¢ 3: Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ")
print("="*60)

query3 = "ĞšÑ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ”Ğ°Ğ¼Ğ±Ğ»Ğ´Ğ¾Ñ€?"
print(f"\nâ“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ: '{query3}'")

show_results(keyword_search(query3, k=3), "1ï¸âƒ£ KEYWORD (BM25)")
show_results(vector_search(query3, k=3), "2ï¸âƒ£ VECTOR (Semantic)")
show_results(hybrid_search(query3, k=3, alpha=0.7), "4ï¸âƒ£ HYBRID (alpha=0.7)")


# ============================================================
# Ğ¡Ğ ĞĞ’ĞĞ•ĞĞ˜Ğ• ĞœĞ•Ğ¢ĞĞ”ĞĞ’
# ============================================================
print("\n\n" + "="*60)
print("ğŸ“Š ĞšĞĞ“Ğ”Ğ ĞšĞĞšĞĞ™ ĞœĞ•Ğ¢ĞĞ” Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ¬")
print("="*60)
print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ĞœĞµÑ‚Ğ¾Ğ´           â”‚ Ğ›ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ´Ğ»Ñ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ KEYWORD (BM25)  â”‚ Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹, Ğ¸Ğ¼ĞµĞ½Ğ°, ĞºĞ¾Ğ´Ñ‹            â”‚
â”‚                 â”‚ "error 404", "Python 3.12"             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VECTOR          â”‚ Ğ¡Ğ¼Ñ‹ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº, ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹              â”‚
â”‚                 â”‚ "ĞºĞ°Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ğ³" â†’ "debugging"      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MMR             â”‚ ĞÑƒĞ¶Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹         â”‚
â”‚                 â”‚ Ğ˜Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HYBRID          â”‚ Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ´Ğ»Ñ production   â”‚
â”‚                 â”‚ ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

