"""
RAG –®–∞–≥ 4: –ë–∞–∑–æ–≤—ã–π RAG Pipeline
===============================
–ü–æ–ª–Ω—ã–π pipeline: Query ‚Üí Search ‚Üí Generate
"""
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

load_dotenv()

# ============================================================
# 1. –ù–ê–°–¢–†–û–ô–ö–ê –ö–û–ú–ü–û–ù–ï–ù–¢–û–í
# ============================================================
print("="*60)
print("üöÄ RAG PIPELINE")
print("="*60)

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.3)
print("‚úÖ LLM: gpt-4.1-mini")

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
print("‚úÖ Embeddings: text-embedding-3-small")


# ============================================================
# 2. –°–û–ó–î–ê–ù–ò–ï/–ó–ê–ì–†–£–ó–ö–ê –ò–ù–î–ï–ö–°–ê
# ============================================================
INDEX_PATH = "./faiss_harry_potter"

if Path(INDEX_PATH).exists():
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞: {INDEX_PATH}")
    vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
else:
    print(f"\nüî® –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
    
    documents = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    
    for file in sorted(Path("data").glob("*.txt")):
        name = file.stem.split(']_')[-1].replace('_', ' ')
        text = file.read_text(encoding='utf-8')
        chunks = splitter.split_text(text)
        
        for i, chunk in enumerate(chunks):
            documents.append(Document(
                page_content=chunk,
                metadata={"title": name, "chunk_id": i}
            ))
        print(f"   üìñ {name}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    print(f"\n   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
    print("   –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –±–∞—Ç—á–∞–º–∏ (–∏–∑–±–µ–≥–∞–µ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤)...")
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –±–∞—Ç—á–∞–º–∏ –ø–æ 500 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    BATCH_SIZE = 500
    vectorstore = None
    
    for i in range(0, len(documents), BATCH_SIZE):
        batch = documents[i:i+BATCH_SIZE]
        print(f"   –ë–∞—Ç—á {i//BATCH_SIZE + 1}: –¥–æ–∫—É–º–µ–Ω—Ç—ã {i}-{i+len(batch)}")
        
        if vectorstore is None:
            vectorstore = FAISS.from_documents(batch, embeddings)
        else:
            batch_store = FAISS.from_documents(batch, embeddings)
            vectorstore.merge_from(batch_store)
    
    vectorstore.save_local(INDEX_PATH)
    print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {INDEX_PATH}")


# ============================================================
# 3. –ü–û–ò–°–ö –ë–ï–ó –ì–ï–ù–ï–†–ê–¶–ò–ò (–¥–µ–º–æ)
# ============================================================
print("\n" + "="*60)
print("üîç –ü–û–ò–°–ö –î–û–ö–£–ú–ï–ù–¢–û–í (–±–µ–∑ LLM)")
print("="*60)

query = "–í–æ–ª–¥–µ–º–æ—Ä—Ç"
print(f"\n‚ùì –ó–∞–ø—Ä–æ—Å: {query}")

docs = vectorstore.similarity_search_with_score(query, k=3)
for doc, score in docs:
    print(f"\nüìÑ [{doc.metadata['title']}] score={score:.3f}")
    print(f"   {doc.page_content[:200]}...")


# ============================================================
# 4. RAG: –ü–û–ò–°–ö + –ì–ï–ù–ï–†–ê–¶–ò–Ø
# ============================================================
print("\n" + "="*60)
print("ü§ñ RAG: –ü–û–ò–°–ö + –ì–ï–ù–ï–†–ê–¶–ò–Ø")
print("="*60)

prompt = ChatPromptTemplate.from_template("""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä—É. –û—Ç–≤–µ—á–∞–π –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:""")


def ask(question: str, k: int = 4) -> str:
    """RAG: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"""
    docs = vectorstore.similarity_search(question, k=k)
    context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    
    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": context, "question": question})
    
    return answer, [doc.metadata['title'] for doc in docs]


# –î–µ–º–æ
questions = [
    "–ö—Ç–æ —Ç–∞–∫–æ–π –í–æ–ª–¥–µ–º–æ—Ä—Ç?",
    "–ö–∞–∫ –ì–∞—Ä—Ä–∏ –ø–æ–ø–∞–ª –≤ –•–æ–≥–≤–∞—Ä—Ç—Å?",
]

for q in questions:
    print(f"\n‚ùì {q}")
    print("-"*50)
    answer, sources = ask(q)
    print(f"üí° {answer}")
    print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(set(sources))}")
